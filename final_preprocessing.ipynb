{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from loadData.ipynb\n",
      "importing Jupyter notebook from Util.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import time\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, hstack\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "\n",
    "from loadData import raw_df, test_df\n",
    "from Util import getAllTypesofCategory, getAllTypesOfProperty\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([raw_df.drop(['is_trade'], axis = 1), test_df], axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## impute missing value with most_frequent strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_missing_value_columns = [\"item_brand_id\",\"item_city_id\",\"item_sales_level\",\"user_gender_id\",\"user_age_level\",\n",
    "                                \"user_occupation_id\",\"user_star_level\",\"shop_review_positive_rate\",\n",
    "                                \"shop_score_service\",\"shop_score_delivery\",\"shop_score_description\"]\n",
    "data_imputer = Imputer(missing_values=-1 , strategy='most_frequent', axis=0)\n",
    "final_df[contain_missing_value_columns] = data_imputer.fit_transform(final_df[contain_missing_value_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['len_item_category'] = final_df['item_category_list'].map(lambda x: len(str(x).split(';')))\n",
    "final_df['len_item_property'] = final_df['item_property_list'].map(lambda x: len(str(x).split(';')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing  predict_category_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predict_category_property_feature(df):\n",
    "    def process_func(item):\n",
    "        result = {}\n",
    "        for i in item.split(';'):\n",
    "            if ':' in i:\n",
    "                items = i.split(':')\n",
    "                category = int(items[0])\n",
    "                properties = items[1]\n",
    "                if ',' in properties:\n",
    "                    result[category] = map(int,items[1].split(','))\n",
    "                else:\n",
    "                    result[category] = [int(items[1])]\n",
    "        return result\n",
    "    def process_category(item):\n",
    "        return list(process_func(item).keys())\n",
    "    def process_property(item):\n",
    "        _ = process_func(item).values()\n",
    "        return list(set(itertools.chain(*_)))\n",
    "    df['predict_categories'] = df['predict_category_property'].apply(lambda item: process_category(item))\n",
    "    df['predict_properties'] = df['predict_category_property'].apply(lambda item: process_property(item))\n",
    "generate_predict_category_property_feature(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = final_df['predict_categories'].values\n",
    "all_categories = list(set(itertools.chain(*tmp)))\n",
    "category_label_encode = LabelEncoder()\n",
    "all_categories_labels = category_label_encode.fit_transform(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['predict_categories'][160943] = ['7908382889764677758']\n",
    "final_df['predict_categories'][205272] = ['7908382889764677758']\n",
    "final_df['predict_categories'][214168] = ['7908382889764677758']\n",
    "final_df['predict_categories'][314685] = ['7908382889764677758']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['predict_categories'] = final_df['predict_categories'].apply(lambda item: category_label_encode.transform(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['item_category_list'] = final_df['item_category_list'].apply(lambda item: item.split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['item_category_list'] = final_df['item_category_list'].apply(lambda item: category_label_encode.transform(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    final_df['predict_categories_%d'%(i)] = final_df['predict_categories'].apply(lambda item:  item[i] if len(item) > i else 0)\n",
    "#     final_df['predict_properties_%d'%(i)] = final_df['predict_properties'].apply(lambda item:  item[i] if len(item) > i else -1)\n",
    "    final_df['item_category_%d'%(i)] = final_df['item_category_list'].apply(lambda x:x[i] if len(x) > i else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_item_property_list_func = lambda item: list(item.split(';'))\n",
    "final_df['item_property_list_array'] = final_df['item_property_list'].apply(lambda item: extract_item_property_list_func(item))\n",
    "\n",
    "arrays = list(final_df['item_property_list_array'])\n",
    "count = collections.Counter(list(itertools.chain(*arrays)))\n",
    "most_common_property_id_list = list(list(zip(*count.most_common(1499)))[0])\n",
    "most_common_property_id_list.append('o')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(most_common_property_id_list)\n",
    "most_common_property_id_num_list = le.transform(most_common_property_id_list)\n",
    "\n",
    "# label property id in item_property_list_array\n",
    "for i in range(final_df.shape[0]):\n",
    "    for index, _id in enumerate(final_df['item_property_list_array'][i]):\n",
    "        if _id not in most_common_property_id_list:\n",
    "            final_df['item_property_list_array'][i][index] = 'o'\n",
    "\n",
    "final_df['item_property_list_array'] = final_df['item_property_list_array'].apply(lambda item:le.transform(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    final_df['item_properties_%d'%(i)] = final_df['item_property_list_array'].apply(lambda x: x[i] if len(x) > i else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = final_df['predict_properties'].values\n",
    "all_properties = list(set(itertools.chain(*tmp)))\n",
    "properties_label_encode = LabelEncoder()\n",
    "all_properties_labels = properties_label_encode.fit_transform(all_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['predict_properties'] = final_df['predict_properties'].apply(lambda item: properties_label_encode.transform(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    final_df['predict_properties_%d'%(i)] = final_df['predict_properties'].apply(lambda x: x[i] if len(x) > i else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'item_category_list', predict_categories\n",
    "def calc_predict_category_accuracy(row_item):\n",
    "    num = 0\n",
    "    predict_categories_list = row_item.predict_categories\n",
    "    for i in row_item.item_category_list:\n",
    "        if i in predict_categories_list:\n",
    "            num += 1\n",
    "    return round(num/len(predict_categories_list), 2)\n",
    "final_df['predict_category_accuracy'] = final_df[['item_category_list', 'predict_categories']].apply((lambda item: calc_predict_category_accuracy(item)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'item_property_list_array', predict_properties\n",
    "# def calc_predict_category_accuracy(row_item):\n",
    "#     num = 0\n",
    "#     predict_properties_list = row_item.predict_categories\n",
    "#     for i in row_item.item_category_list:\n",
    "#         if i in predict_categories_list:\n",
    "#             num += 1\n",
    "#     return round(num/len(predict_categories_list), 2)\n",
    "# final_df['predict_category_accuracy'] = final_df[['item_category_list', 'predict_categories']].apply((lambda item: calc_predict_category_accuracy(item)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need_onehot_columns = ['item_brand_id','item_city_id','user_gender_id', 'predict_categories_0',\n",
    "#        'item_category_0', 'predict_categories_1', 'item_category_1',\n",
    "#        'predict_categories_2', 'item_category_2', 'item_properties_0',\n",
    "#        'item_properties_1', 'item_properties_2', 'item_properties_3',\n",
    "#        'item_properties_4', 'predict_properties_0', 'predict_properties_1',\n",
    "#        'predict_properties_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_label_oh_enc = OneHotEncoder()\n",
    "# category_label_oh_enc.fit(all_categories_labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_categories_df = final_df['predict_categories'].apply(lambda item: np.sum(category_label_oh_enc.transform(item.reshape(-1,1)),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_npz('../total_data/predict_categories.npz',  csr_matrix(np.concatenate(predict_categories_df.values).reshape(len(predict_categories_df),-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing the object column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need_onehot_columns = ['item_brand_id','item_city_id','user_gender_id','item_one_level_category', 'item_second_level_category']\n",
    "# special_onehot_columns = ['item_property_list_array', 'item_third_level_category']\n",
    "# other_columns = ['context_timestamp','predict_category_property','is_trade']\n",
    "# need_scale_columns = ['item_price_level','item_sales_level','item_collected_level','item_pv_level',\n",
    "#                      'user_age_level','user_occupation_id','user_star_level','context_page_id',\n",
    "#                      'shop_review_num_level','shop_review_positive_rate','shop_star_level','shop_score_service',\n",
    "#                     'shop_score_description', 'day', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = OneHotEncoder()\n",
    "# enc.fit(most_common_property_id_num_list.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tmp_df = raw_df['item_property_list_array'].apply(lambda item: np.sum(enc.transform(item.reshape(-1,1)),axis=0).reshape(1,-1)[0])\n",
    "# save_npz('../total_data/item_property.npz', csr_matrix(np.concatenate(_tmp_df.values).reshape(len(_tmp_df),-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_id_df = final_df['instance_id']\n",
    "data_df = final_df.drop(['instance_id', 'item_category_list', 'item_brand_id','item_city_id','user_gender_id', 'item_property_list', 'context_timestamp', 'time', 'predict_properties', 'predict_categories', 'predict_category_property', 'item_property_list_array'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard level data\n",
    "for i, column in zip(range(1, 6), ['user_age_level', 'user_occupation_id', 'user_star_level', 'context_page_id', 'shop_star_level']):\n",
    "    data_df[column] = data_df[column].apply(lambda item: item%(i*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encode item_brand_id, item_city_id, user_genger_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = csr_matrix(data_df.as_matrix())\n",
    "for column in ['item_brand_id','item_city_id','user_gender_id']:\n",
    "    lb = LabelEncoder()\n",
    "    ohenc = OneHotEncoder()\n",
    "    result_data = hstack((ohenc.fit_transform(lb.fit_transform(final_df[column]).reshape(-1,1)), result_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data into train valid and test set, shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, valid_index, test_index = max(data_df[data_df.day < 24].index)+1, max(data_df[data_df.day == 24].index)+1,max(data_df[data_df.day == 25].index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = result_data.tocsr()\n",
    "train_X, valid_X, test_X = _data[:train_index, :], _data[train_index: valid_index, :], _data[valid_index:, :]\n",
    "train_Y, valid_Y  = raw_df[raw_df.day < 24]['is_trade'].as_matrix(), raw_df[raw_df.day == 24]['is_trade'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = shuffle(train_X, train_Y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test7 = {\n",
    "# 'n_estimators':[20]\n",
    "# }\n",
    "# clf = GridSearchCV(lgb.LGBMClassifier(objective='binary',\n",
    "#      num_leaves=64,\n",
    "#      learning_rate=0.01,\n",
    "#      n_estimators=2000,\n",
    "#      max_depth=7,\n",
    "#      min_samples_split=100),\n",
    "#      param_test7,\n",
    "#      verbose=1,\n",
    "#      cv=5,\n",
    "#      scoring='log_loss',\n",
    "#      n_jobs=4)\n",
    "# clf.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gbm = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "#          learning_rate=0.01, max_depth=7, min_child_samples=20,\n",
    "#          min_child_weight=0.001, min_samples_split=100, min_split_gain=0.0,\n",
    "#          n_estimators=2000, n_jobs=-1, num_leaves=64, objective='binary',\n",
    "#          random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
    "#          subsample=1.0, subsample_for_bin=200000, subsample_freq=1)\n",
    "# evals_result = {}\n",
    "# gbm.fit(train_X, train_Y,\n",
    "# #         feature_name = list(trainX.columns.values),\n",
    "#         eval_set=[(valid_X, valid_Y)],\n",
    "#         eval_metric='binary_logloss',\n",
    "#         early_stopping_rounds=100\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_properties = gbm.predict_proba(test_X)[:, 1]\n",
    "# result_df = pd.concat([test_df['instance_id'], pd.DataFrame(data=result_properties, columns=['predicted_score'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.to_csv('../total_data/baseline_20180327_0821885.txt', index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.fit(train_X, train_Y)\n",
    "# print(log_loss(valid_Y, lr.predict_proba(valid_X)[:, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
